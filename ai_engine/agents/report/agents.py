from ..base import BaseAgent
from langchain_core.messages import SystemMessage, HumanMessage

class ScientificWritingAgent(BaseAgent):
    def __init__(self, **kwargs):
        super().__init__(
            name="ScientificWriting",
            system_prompt="""You are a Lead Research Scientist writing a Major Review Paper (Target: 10,000+ words). 
            Synthesize all findings into a MASSIVE, cohesive, academic report.
            
            Strategy for Depth (15+ Pages):
            1. **Abstract**: Concise (300 words).
            2. **Introduction**: Historical context, evolution (2 pages).
            3. **Literature Review**: Do not just list papers. Analyse themes, conflicts, and methodologies details for ALL 30+ sources (5-6 pages).
            4. **Methodology**: Technical deep dive (3 pages).
            5. **Results & Novelty**: Detailed comparisons (3 pages).
            6. **Discussion**: Implications, ethics, future work (2 pages).

            **Style**:
            - Use Academic English (Formal).
            - Use dense paragraphs.
            - Use mathematical notations where possible ($E=mc^2$).
            
            **Visualization**:
            - EMBED the Mermaid charts in the appropriate sections using ```mermaid ... ``` blocks.
            """,
            **kwargs
        )

    def run(self, state: dict) -> dict:
        print(f"[{self.name}] Writing Comprehensive Report...")
        findings = state.get("findings", {})
        
        # Flatten findings into a single massive context string
        context_parts = []
        for key, value in findings.items():
            if key.startswith("_"): continue # skip headers
            context_parts.append(f"SECTION [{key.upper()}]:\n{value}")
            
        full_context = "\n\n".join(context_parts)
        
        # We need to ensure we don't blow up the context window too bad, but Mistral/Llama usually handle 32k or so.
        # We truncate safely just in case
        safe_context = full_context[:25000]
        
        enhanced_prompt = f"{self.system_prompt}\n\nRESEARCH DATA AGGREGATION:\n{safe_context}"
        
        # Inject Generated Image Context
        if "visualization" in findings:
            viz = findings["visualization"]
            if isinstance(viz, dict) and "generated_image_path" in viz:
                img_path = viz["generated_image_path"]
                enhanced_prompt += f"\n\n[IMPORTANT] A custom AI-generated visualization is available at '{img_path}'. You MUST include it in the report using standard Markdown: ![AI Generated Visualization]({img_path})\n"
        
        messages = [
            SystemMessage(content=enhanced_prompt + "\n\nIMPORTANT: Output Markdown ONLY."),
            HumanMessage(content="Write the Full Paper now.")
        ]
        
        try:
            response = self.llm.invoke(messages)
            # This agent likely returns text, not JSON, but let's wrap it in our structure
            return {
                "response": {"markdown_report": response.content},
                "raw": response.content,
                "agent": self.name
            }
        except Exception as e:
            print(f"[{self.name}] Error: {e}")
            return {"error": str(e)}

class LaTeXGenerationAgent(BaseAgent):
    def __init__(self, **kwargs):
        super().__init__(
            name="LaTeXGeneration",
            system_prompt="""You are a LaTeX Typesetting Expert. 
            Convert the provided Markdown Research Report into a Professional Academic PDF Source (LaTeX).
            
            **Layout Requirements (15-20 Pages Target)**:
            - Class: `\\documentclass[12pt, a4paper]{article}`
            - Packages: `geometry` (1 inch margins), `times` (font), `graphicx`, `hyperref`, `amsmath`, `fancyhdr`.
            - Structure:
              - File must include `\\tableofcontents` and `\\newpage` after sections to expand length.
              - Use `\\section{}`, `\\subsection{}`, `\\subsubsection{}` deeply.
            - Notations: Ensure all math $...$ is preserved and formatted.
            - Header/Footer: Use `fancyhdr` to add "Generated by AI Research Engine" in footer and Page Numbers.
            
            Output RAW LaTeX code only.
            """,
            **kwargs
        )

    def run(self, state: dict) -> dict:
        print(f"[{self.name}] Generating LaTeX Source...")
        findings = state.get("findings", {})
        
        markdown_content = ""
        if "scientific_writing" in findings:
            markdown_content = findings["scientific_writing"].get("markdown_report", "")
        
        if not markdown_content:
            return {"error": "No markdown report found to convert."}
            
        enhanced_prompt = f"{self.system_prompt}\n\nSOURCE MARKDOWN:\n{markdown_content[:25000]}"
        
        messages = [
            SystemMessage(content=enhanced_prompt + "\n\nIMPORTANT: Output LaTeX Code ONLY (no ```latex blocks if possible, just code)."),
            HumanMessage(content="Generate the LaTeX now.")
        ]
        
        try:
            response = self.llm.invoke(messages)
            return {
                "response": {"latex_source": response.content},
                "raw": response.content,
                "agent": self.name
            }
        except Exception as e:
            print(f"[{self.name}] Error: {e}")
            return {"error": str(e)}
