================================================================================
    INDUSTRY-GRADE AI RESEARCH PLATFORM — SYSTEM ARCHITECTURE REDESIGN
    Multi-Agent LangGraph-Orchestrated LLM Research Automation Platform
================================================================================
    Generated: 2026-02-25
================================================================================


=== ROOT CAUSE ANALYSIS ===

Current Issues Identified:
1. worker.js calls recoverStaleJobs() on startup → re-queues old 'processing'
   jobs → processQueue() picks them up immediately without user action.
2. No workspace concept in database — research_logs only has user_id.
3. No session guard — worker processes ANY queued job blindly.
4. state_store.py uses global in-memory dict with no workspace scoping.


================================================================================
1. SYSTEM ARCHITECTURE OVERVIEW
================================================================================

Architecture Type: Modular Monolith (3-Tier)

Tier 1 — Frontend (React 19 + Vite + TypeScript)
  ├── Landing Page
  ├── Auth Pages (Login / Signup)
  ├── Workspace List Page (NEW)
  ├── Workspace View (Research + Chat + Upload)
  └── Shared Research View

Tier 2 — Backend (Node.js + Express)
  ├── Auth API (JWT)
  ├── Workspace API (NEW — CRUD)
  ├── Research API (Session lifecycle)
  ├── Chat API (Workspace-scoped RAG)
  ├── Events API (SSE streaming)
  ├── Export API (PDF / LaTeX)
  └── Worker (Job Queue Processor)

Tier 3 — AI Engine (Python + FastAPI)
  ├── LangGraph Pipeline (27 Agents)
  ├── RAG / Retrieval Module (NEW)
  ├── Vector Store Manager (NEW — ChromaDB / pgvector)
  ├── File Processing Pipeline (NEW — PDF parsing, chunking)
  └── Session State Store (Redis-backed, replaces in-memory dict)

Storage Layer:
  ├── PostgreSQL (Primary database)
  ├── Redis (Session state + Cache)
  ├── ChromaDB / pgvector (Vector embeddings)
  └── File System (Research artifacts, uploads)


================================================================================
2. EXECUTION LIFECYCLE (STRICT FLOW)
================================================================================

Step 1: User Sign In
  → POST /auth/login
  → Returns JWT token

Step 2: Select or Create Workspace
  → GET /workspaces (list existing)
  → POST /workspaces (create new — clean state)
  → Each workspace has: UUID, name, description, isolated context

Step 3: Enter Topic + Click "Start Research"
  → POST /workspaces/:wid/research/start {topic, depth}
  → Backend validates: auth token + workspace ownership
  → INSERT research_sessions (status='queued', trigger_source='user')
  → pg_notify('new_job') — wakes up worker
  → Returns: {session_id, status: 'queued'}

Step 4: Worker Processes Job
  → Worker picks up queued job (FOR UPDATE SKIP LOCKED)
  → Marks status = 'processing'
  → Sends to AI Engine: POST /research {session_id, workspace_id, topic}
  → 27-agent LangGraph pipeline runs:
      Topic Discovery → Topic Lock → Orchestrator
        → Pipeline A: Domain, Historical, SLR, News, Gap, Innovation
        → Pipeline B: Decomposition, Understanding, Verification, Critique
        → Shared: Visualization, Report Writing, LaTeX, Scoring
  → Results stored with workspace scope
  → Embeddings stored in workspace-specific vector collection

Step 5: Real-Time Progress (SSE)
  → GET /events/:session_id
  → Streams execution_events from AI Engine
  → Frontend shows live agent activity

Step 6: Retrieval & Q&A (Chat)
  → POST /workspaces/:wid/chat {message, session_id?}
  → Backend forwards to AI Engine
  → AI Engine performs workspace-scoped vector retrieval
  → LLM generates RAG-enhanced response with citations
  → Streaming response back to frontend

Step 7: Optional Paper Generation
  → Workspace view has "Generate Paper" button
  → Triggers LaTeX compilation pipeline
  → PDF rendered and available for download

KEY CONSTRAINT ENFORCED:
  Sign In → Select/Create Workspace → Enter Topic → Start Research
  → Store Context → Enable Retrieval → Optional Paper Generation

  NO research begins before workspace initialization and explicit user trigger.


================================================================================
3. DATABASE SCHEMA REDESIGN
================================================================================

--- NEW TABLE: workspaces ---

  CREATE TABLE workspaces (
      id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
      name VARCHAR(255) NOT NULL,
      description TEXT,
      status VARCHAR(20) DEFAULT 'active',   -- active | archived
      created_at TIMESTAMP DEFAULT NOW(),
      updated_at TIMESTAMP DEFAULT NOW()
  );

--- NEW TABLE: research_sessions (replaces research_logs) ---

  CREATE TABLE research_sessions (
      id SERIAL PRIMARY KEY,
      workspace_id UUID REFERENCES workspaces(id) ON DELETE CASCADE,
      user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
      topic TEXT NOT NULL,
      refined_topic TEXT,
      status VARCHAR(50) DEFAULT 'queued',
      trigger_source VARCHAR(20) DEFAULT 'user',  -- 'user' | 'system' | 'retry'
      depth VARCHAR(20) DEFAULT 'deep',
      result_json JSONB,
      retry_count INTEGER DEFAULT 0,
      current_stage VARCHAR(50) DEFAULT 'queued',
      started_at TIMESTAMP,
      completed_at TIMESTAMP,
      created_at TIMESTAMP DEFAULT NOW(),
      updated_at TIMESTAMP DEFAULT NOW()
  );

--- NEW TABLE: workspace_uploads ---

  CREATE TABLE workspace_uploads (
      id SERIAL PRIMARY KEY,
      workspace_id UUID REFERENCES workspaces(id) ON DELETE CASCADE,
      user_id INTEGER REFERENCES users(id),
      filename VARCHAR(500) NOT NULL,
      file_type VARCHAR(50),
      file_path TEXT NOT NULL,
      file_size_bytes BIGINT,
      embedding_status VARCHAR(20) DEFAULT 'pending',
      chunk_count INTEGER DEFAULT 0,
      created_at TIMESTAMP DEFAULT NOW()
  );

--- NEW TABLE: embeddings_meta ---

  CREATE TABLE embeddings_meta (
      id SERIAL PRIMARY KEY,
      workspace_id UUID REFERENCES workspaces(id) ON DELETE CASCADE,
      session_id INTEGER REFERENCES research_sessions(id),
      source_type VARCHAR(50),
      source_url TEXT,
      chunk_index INTEGER,
      vector_id VARCHAR(255),
      content_preview TEXT,
      created_at TIMESTAMP DEFAULT NOW()
  );

--- MODIFIED: chat_history ---
  ADD COLUMN workspace_id UUID REFERENCES workspaces(id);
  ADD COLUMN session_id INTEGER REFERENCES research_sessions(id);

--- MODIFIED: execution_events ---
  RENAME COLUMN research_id TO session_id;

--- MODIFIED: data_sources ---
  ADD COLUMN workspace_id UUID REFERENCES workspaces(id);
  RENAME COLUMN research_id TO session_id;

--- MODIFIED: user_memories ---
  ADD COLUMN workspace_id UUID REFERENCES workspaces(id);

EXISTING TABLES (no changes):
  - users
  - api_keys


================================================================================
4. API STRUCTURE
================================================================================

Auth Routes:
  POST /auth/login                              No auth required
  POST /auth/signup                             No auth required

Workspace Routes (NEW):
  GET    /workspaces                            List user workspaces
  POST   /workspaces                            Create workspace
  GET    /workspaces/:wid                       Get workspace details
  PATCH  /workspaces/:wid                       Update workspace
  DELETE /workspaces/:wid                       Archive workspace

Research Routes (MODIFIED):
  POST   /workspaces/:wid/research/start        Start research (explicit trigger)
  GET    /workspaces/:wid/research/:sid/status   Check session status
  POST   /workspaces/:wid/research/:sid/topic    Lock topic selection
  GET    /workspaces/:wid/research/:sid/suggest  Get topic suggestions

Chat Routes (MODIFIED):
  POST   /workspaces/:wid/chat                  Send chat message (RAG)
  GET    /workspaces/:wid/chat/history           Get chat history

Upload Routes (NEW):
  POST   /workspaces/:wid/upload                Upload file (PDF/dataset)
  GET    /workspaces/:wid/uploads                List uploads

Source Routes:
  GET    /workspaces/:wid/sources               List scraped sources

Export Routes:
  GET    /workspaces/:wid/export/pdf             Export as PDF
  GET    /workspaces/:wid/export/latex           Export as LaTeX

Events:
  GET    /events/:sid                            SSE stream for session


================================================================================
5. RESEARCH PIPELINE REQUIREMENTS
================================================================================

A. Scraping Targets (20–50 URLs per topic):

  Current Providers (6):
    - DuckDuckGo (general web)
    - Google Search (comprehensive)
    - ArXiv (academic papers)
    - Wikipedia (encyclopedia)
    - OpenAlex (scientific literature)
    - PubMed (medical/life science)

  Recommended Additions:
    - Google Scholar via SerpAPI (citation chaining)
    - Semantic Scholar API (related papers)

  Configuration: max_results per provider = 5-10 → total 40-80 URLs

B. Resource Extraction (50+ resources per topic):

  From each URL:
    - Source metadata (title, authors, date, domain, URL)
    - Full text extraction (via web scraper agent)
    - LLM-generated summary (200-500 words)
    - Embedding generation (sentence-transformers)
    - Images extraction (if relevant)
    - Citation text (for academic sources)

  Resource Types:
    - Surveys / Reviews
    - Research papers
    - Articles / Blog posts
    - Technical documentation
    - News articles
    - Wikipedia entries
    - Images / Figures

C. Research Strategies:

  1. Breadth-first (default):
     Hit all providers simultaneously, aggregate top results

  2. Citation chaining:
     Take top-5 papers → extract references → scrape referenced papers
     (via Semantic Scholar API or Google Scholar)

  3. Semantic expansion:
     Embed initial findings → find similar content via vector similarity
     → scrape top-k similar but undiscovered sources


================================================================================
6. RETRIEVAL & QUESTION ANSWERING SYSTEM
================================================================================

Primary Goal: Build large contextual knowledge base per workspace,
enabling deep Q&A across ALL scraped + uploaded content.

Architecture:

  User Question
      ↓
  Query Router (classifies intent)
      ↓
  ┌─────────────────┬─────────────────┬──────────────────┐
  │ Semantic Search  │ Citation Lookup │ Hybrid Retrieval │
  │  (Vector DB)     │  (Source DB)    │  (BM25 + Vector) │
  └─────────────────┴─────────────────┴──────────────────┘
      ↓                     ↓                    ↓
  Cross-Encoder Reranker (top-k=10 → top-5)
      ↓
  LLM with RAG Context
      ↓
  Cited Answer (with [Source N] references)

Implementation Details:

  Vector Database: ChromaDB (local) or pgvector (PostgreSQL extension)
  Embedding Model: sentence-transformers/all-MiniLM-L6-v2 (free, fast)
  Chunking: 512 tokens, 64-token overlap
  Retrieval: Top-k=10 → rerank → top-5 to LLM context

  Retrieval Modes:
    a) Semantic search (default) — cosine similarity on embeddings
    b) Citation-based lookup — by paper/source ID from database
    c) Hybrid — BM25 full-text search + vector similarity

  User Uploads:
    PDFs parsed → text extracted → chunked → embedded → stored in
    workspace-specific vector collection → available for retrieval


================================================================================
7. RESEARCH OUTPUT SYSTEM
================================================================================

Primary Focus: Data gathering + retrieval intelligence
Secondary (Optional): Structured research paper generation

Output Pipeline:
  1. Multi-stage report generation (existing agents)
     - Scientific Writing Agent → structured markdown report
     - LaTeX Generation Agent → professional typesetting
     - LaTeX Compilation → PDF rendering

  2. Frontend Display:
     - Markdown report rendered in workspace view
     - PDF download button
     - LaTeX source download
     - Individual section export

  3. Quality Controls (existing):
     - Adversarial Critique Agent
     - Hallucination Detection Agent
     - Technical Verification Agent
     - Research Scoring Agent


================================================================================
8. WORKER HARDENING (CRITICAL FIX)
================================================================================

Current Bug:
  worker.js → startWorker() → recoverStaleJobs() → re-queues old jobs
  → processQueue() picks them up → research auto-starts

Fix:

  1. recoverStaleJobs() ONLY recovers jobs WHERE:
     - trigger_source = 'user'
     - status = 'processing' (not 'queued')
     - updated_at < NOW() - 30 minutes
     - retry_count < MAX_RETRIES

  2. processQueue() ONLY picks up jobs WHERE:
     - trigger_source = 'user'
     - status = 'queued'

  3. On system restart:
     - Do NOT auto-queue anything
     - Mark stale 'processing' jobs as 'stale' (new status)
     - User must explicitly re-trigger stale research from frontend

  4. New job statuses:
     queued → processing → completed
                        → failed
                        → stale (system restart detected)


================================================================================
9. STATE MANAGEMENT & SESSION ISOLATION
================================================================================

Current Problem:
  state_store.py uses:
    RESEARCH_STATES = {}  # Global in-memory dict, lost on restart

New Design:
  Redis-backed state store with workspace scoping

  Key Format: "research:{session_id}:state"
  TTL: 24 hours auto-expiry

  Benefits:
    - Survives process restarts
    - Workspace-scoped (no cross-contamination)
    - Supports multiple worker instances
    - Automatic cleanup of old sessions


================================================================================
10. CACHING STRATEGY
================================================================================

  Layer              │ Tool            │ TTL       │ Purpose
  ────────────────── │ ─────────────── │ ───────── │ ──────────────────────
  Search results     │ Redis           │ 1 hour    │ Avoid duplicate API calls
  LLM responses      │ Redis           │ 24 hours  │ Cache repeated queries
  Vector embeddings  │ ChromaDB/pgvec  │ Permanent │ Per-workspace retrieval
  Session state      │ Redis           │ 24 hours  │ Pipeline state isolation
  Rate limit counters│ Redis           │ 15 min    │ Rate limiting


================================================================================
11. SECURITY CONSIDERATIONS
================================================================================

  Authentication:
    - JWT tokens with expiry (24h)
    - Refresh token rotation
    - Password hashing (bcrypt)

  Authorization:
    - Workspace ownership middleware
    - User can ONLY access their own workspaces
    - API key validation for AI engine internal calls

  Input Validation:
    - Topic sanitization (XSS prevention)
    - File upload type/size limits (PDF: 50MB, CSV: 10MB)
    - Rate limiting per user per workspace

  Data Isolation:
    - All queries include workspace_id filter
    - Vector DB collections are workspace-scoped
    - Chat history is workspace-scoped
    - No cross-workspace data leakage


================================================================================
12. PRODUCTION DEPLOYMENT
================================================================================

Docker Compose Services:
  1. postgres (PostgreSQL 16 + pgvector extension)
  2. redis (State store + Cache)
  3. backend (Node.js API)
  4. worker (Job processor — scalable 1-N instances)
  5. ai_engine (FastAPI + LangGraph)
  6. frontend (Vite production build → nginx)
  7. chromadb (Vector database, if not using pgvector)

Environment Configuration:
  - .env.development / .env.production
  - DATABASE_URL, REDIS_URL, AI_ENGINE_URL
  - LLM API keys (Gemini, Groq, Ollama)
  - JWT_SECRET, AI_ENGINE_SECRET

Scaling Recommendations:
  - Worker: Scale horizontally (multiple instances)
  - AI Engine: 2-4 workers behind load balancer
  - PostgreSQL: Connection pooling (PgBouncer)
  - Redis: Sentinel for HA
  - Vector DB: Persistent volume


================================================================================
13. SUGGESTED TECHNOLOGY STACK
================================================================================

  Component               │ Technology
  ─────────────────────── │ ──────────────────────────────────
  Frontend                │ React 19, TypeScript, Vite, Zustand
  UI Framework            │ Tailwind CSS, Shadcn/UI
  Backend API             │ Node.js 20, Express.js
  AI Engine               │ Python 3.11, FastAPI, LangGraph
  LLM Providers           │ Gemini 2.0 Flash, Groq, Ollama
  Primary Database        │ PostgreSQL 16
  Cache / State Store     │ Redis 7
  Vector Database         │ ChromaDB (or pgvector extension)
  Embedding Model         │ sentence-transformers/all-MiniLM-L6-v2
  Task Queue              │ PG LISTEN/NOTIFY + Worker polling
  File Processing         │ PyPDF2, pdfplumber
  Text Chunking           │ LangChain RecursiveCharacterTextSplitter
  Containerization        │ Docker, Docker Compose
  CI/CD                   │ GitHub Actions


================================================================================
14. IMPLEMENTATION PHASES
================================================================================

  Phase 1 (2-3 days):
    - Database migration (new tables, modified columns)
    - Workspace CRUD API + frontend workspace list page
    - Basic workspace selection flow

  Phase 2 (2-3 days):
    - Research session lifecycle (explicit trigger)
    - Worker hardening (no auto-start)
    - State management with Redis

  Phase 3 (2-3 days):
    - Vector DB integration (ChromaDB / pgvector)
    - Embedding pipeline (scrape → chunk → embed → store)
    - Workspace-scoped collections

  Phase 4 (2-3 days):
    - RAG retrieval system
    - Chat scoping to workspace
    - File upload + PDF processing

  Phase 5 (1-2 days):
    - Research output (LaTeX/PDF)
    - Frontend polish
    - Production Docker configuration

  TOTAL ESTIMATED: 10–14 days


================================================================================
END OF DOCUMENT
================================================================================
