# ===========================================
# Multi-Agent LLM Research Automation Platform
# Environment Variables Configuration
# ===========================================

# ===========================================
# [Frontend] (React/Vite Next.js config)
# ===========================================
NEXT_PUBLIC_API_URL=http://localhost:5000/api
NEXT_PUBLIC_WS_URL=ws://localhost:5000/events
NEXTAUTH_URL=http://localhost:3000
NEXTAUTH_SECRET=generate_strong_secret

# Legacy variables (if still used by Vite)
VITE_API_BASE_URL=http://localhost:5000
VITE_AI_ENGINE_URL=http://127.0.0.1:8000

# ===========================================
# [Backend] (Node.js/Express)
# ===========================================
NODE_ENV=development
PORT=5000
FRONTEND_URL=http://localhost:3000
API_BASE_URL=http://localhost:5000

# Database Configuration (PostgreSQL)
# Use DATABASE_URL for Aiven/Supabase (cloud)
# DATABASE_URL=postgres://user:password@host:port/dbname?sslmode=require

# Or use split variables for local offline postgres:
DB_HOST=localhost
DB_PORT=5432
DB_NAME=research_platform
DB_USER=sans
DB_PASSWORD=your_db_password

# External Services
AI_ENGINE_URL=http://127.0.0.1:8000
UPSTASH_REDIS_REST_URL=
UPSTASH_REDIS_REST_TOKEN=

# Authentication & Security
JWT_SECRET=your_jwt_secret_key_here
JWT_EXPIRES_IN=7d
SESSION_SECRET=another_strong_random_string

# OAuth Credentials
GITHUB_CLIENT_ID=
GITHUB_CLIENT_SECRET=
GOOGLE_CLIENT_ID=
GOOGLE_CLIENT_SECRET=

# Security & Traffic Control
CORS_ORIGINS=http://localhost:3000,https://multiagent-lang-graph-orchestrated.vercel.app
RATE_LIMIT_REQUESTS_PER_MIN=60
LOG_LEVEL=info

# Worker Queue
QUEUE_CONCURRENCY=2

# Internal API Security (must match AI_ENGINE_SECRET below)
AI_ENGINE_SECRET=change_this_to_a_secure_random_string

# ===========================================
# [AI Engine] (Python/FastAPI)
# ===========================================
ENVIRONMENT=development

# LLM Mode: "OFFLINE" (Ollama local models) or "ONLINE" (OpenRouter/Groq/Gemini cloud APIs)
LLM_STATUS=OFFLINE

# --- API Keys for Online Mode ---
# OpenRouter (Reasoning)
OPENROUTER_API_1=
OPENROUTER_API_2=
OPENROUTER_API_3=

# Groq (Fast Drafting / Synthesis)
GROQ_API_1=
GROQ_API_2=
GROQ_API_3=

# Gemini (Fallback / General)
GEMINI_API_1=
GEMINI_API_2=
GEMINI_API_3=

# Hugging Face (Embeddings / specialized tasks)
HF_TOKEN=

# --- Offline Model Settings ---
OLLAMA_BASE_URL=http://localhost:11434

# --- Specialized Model Configuration ---
MODEL_REASONING=openrouter/anthropic/claude-3.5-sonnet:beta
MODEL_WRITING=groq/gemma2-9b-it
MODEL_CODING=qwen2.5-coder:1.5b
MODEL_CRITICAL=gemini/gemini-2.0-flash

# Limits & Concurrency
MAX_TOKENS=4096
AI_ENGINE_MAX_WORKERS=4

# Search Providers
GOOGLE_SEARCH_API_KEY=
GOOGLE_SEARCH_CX=

# Database / Redis (For State Store & Status updates)
# Uses same DATABASE_URL as Backend
REDIS_URL=