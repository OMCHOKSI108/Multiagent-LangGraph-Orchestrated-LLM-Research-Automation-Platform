/**
 * Mock data for Demo Mode.
 * When VITE_DEMO_MODE=true the frontend uses this data instead of calling a real backend.
 * Only the demo account (devang@gmail.com / OMchoksi@30) is accepted.
 */

// â”€â”€â”€ Demo User â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const DEMO_EMAIL = 'devang@gmail.com';
export const DEMO_PASSWORD = 'OMchoksi@30';
export const DEMO_USER = { id: '1', email: DEMO_EMAIL, name: 'Devang Dhandhukiya' };
export const DEMO_TOKEN = 'demo-jwt-token-mock-xxxx';

// â”€â”€â”€ Mermaid Diagrams â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const timelineMermaid = `gantt
  title Research Timeline
  dateFormat  YYYY-MM-DD
  section Discovery
  Literature Search     :done, d1, 2026-01-15, 3d
  Domain Analysis       :done, d2, after d1, 2d
  section Review
  Paper Analysis        :done, r1, after d2, 4d
  Gap Identification    :done, r2, after r1, 2d
  section Synthesis
  Report Writing        :done, s1, after r2, 3d
  LaTeX Generation      :done, s2, after s1, 1d`;

const methodologyMermaid = `flowchart TD
  A[Research Question] --> B[Literature Review]
  B --> C[Data Collection]
  C --> D[Deep Learning Models]
  D --> E{Model Evaluation}
  E -->|Accuracy > 90%| F[Deploy Model]
  E -->|Accuracy < 90%| G[Hyperparameter Tuning]
  G --> D
  F --> H[Clinical Validation]
  H --> I[Report Generation]`;

const dataChartMermaid = `pie title Research Sources Distribution
  "Arxiv Papers" : 35
  "PubMed Articles" : 25
  "Google Scholar" : 20
  "Wikipedia" : 10
  "News Articles" : 10`;

// â”€â”€â”€ Mock Report Markdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const reportMarkdown1 = `# Transformer Architectures for Natural Language Processing: A Comprehensive Survey

*Generated by AI Research Engine â€” Computer Science / NLP Format*

*Date: 2026-02-18*

---

## Abstract

The transformer architecture has revolutionized natural language processing (NLP) since its introduction in 2017. This comprehensive survey examines the evolution, variants, and applications of transformer models across diverse NLP tasks. We analyze key architectural innovations including self-attention mechanisms, positional encodings, and scaling laws that have enabled models like BERT, GPT, and T5 to achieve state-of-the-art performance. Our findings indicate that transformer-based models consistently outperform traditional recurrent architectures by 15-30% on standard benchmarks. We identify critical challenges including computational costs, environmental impact, and hallucination issues, while proposing future research directions in efficient transformers, multimodal integration, and ethical AI deployment.

**Keywords:** Transformer, Self-Attention, BERT, GPT, NLP, Deep Learning, Language Models

---

## 1. Introduction

Natural Language Processing has undergone a paradigm shift with the introduction of the transformer architecture by Vaswani et al. (2017). Unlike recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, transformers process all input tokens in parallel through self-attention mechanisms, enabling substantially faster training on modern hardware.

The impact has been profound: pre-trained transformer models now serve as the foundation for virtually all state-of-the-art NLP systems. BERT (Devlin et al., 2019) introduced bidirectional pre-training, while the GPT family (Radford et al., 2018; Brown et al., 2020; OpenAI, 2023) demonstrated the power of autoregressive language modeling at scale.

### 1.1 Research Objectives

This survey aims to:
1. Provide a comprehensive taxonomy of transformer variants
2. Analyze performance comparisons across standard NLP benchmarks
3. Identify current limitations and open challenges
4. Propose actionable future research directions

### 1.2 Scope and Methodology

We systematically reviewed 150+ papers from top-tier venues (ACL, EMNLP, NeurIPS, ICML) published between 2017-2026. Our analysis covers architectural innovations, training methodologies, and application domains.

---

## 2. Background and Related Work

### 2.1 Pre-Transformer Era

Before transformers, NLP relied heavily on:
- **Word2Vec** (Mikolov et al., 2013): Static word embeddings
- **LSTMs** (Hochreiter & Schmidhuber, 1997): Sequential processing with gating
- **Seq2Seq** (Sutskever et al., 2014): Encoder-decoder frameworks with attention

### 2.2 The Attention Revolution

Bahdanau et al. (2015) introduced attention mechanisms for neural machine translation, enabling models to focus on relevant input tokens. This paved the way for the self-attention mechanism in transformers.

| Model | Year | Parameters | GLUE Score |
|-------|------|-----------|------------|
| ELMo | 2018 | 94M | 68.7 |
| BERT-Base | 2019 | 110M | 79.6 |
| GPT-2 | 2019 | 1.5B | 72.5 |
| T5-Large | 2020 | 770M | 88.5 |
| GPT-4 | 2023 | ~1.8T | 92.1 |
| Gemini Ultra | 2024 | ~1.5T | 93.4 |

---

## 3. Transformer Architecture Analysis

### 3.1 Self-Attention Mechanism

The core innovation of transformers is scaled dot-product attention:

$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$

Where Q (queries), K (keys), and V (values) are linear projections of the input. Multi-head attention extends this by running h parallel attention functions.

### 3.2 Positional Encoding

Since transformers lack inherent sequential ordering, positional encodings are added:
- **Sinusoidal** (Vaswani et al., 2017): Fixed frequency-based encodings
- **Learned** (Devlin et al., 2019): Trainable position embeddings  
- **RoPE** (Su et al., 2021): Rotary Position Embeddings enabling length extrapolation
- **ALiBi** (Press et al., 2022): Attention with Linear Biases

### 3.3 Key Architectural Variants

| Architecture | Type | Key Innovation | Notable Models |
|---|---|---|---|
| Encoder-only | Bidirectional | Masked Language Modeling | BERT, RoBERTa |
| Decoder-only | Autoregressive | Causal Language Modeling | GPT, LLaMA |
| Encoder-Decoder | Seq2Seq | Span Corruption | T5, BART |
| Mixture of Experts | Sparse | Conditional Computation | Mixtral, Switch |

---

## 4. Experimental Results

### 4.1 Benchmark Performance

Our meta-analysis across 50 studies reveals consistent trends:

| Task | Best Transformer | Best Non-Transformer | Improvement |
|------|-----------------|---------------------|-------------|
| Text Classification | 96.8% | 89.2% | +7.6% |
| Named Entity Recognition | 94.6% | 88.1% | +6.5% |
| Machine Translation (BLEU) | 46.4 | 35.8 | +10.6 |
| Question Answering (F1) | 93.2% | 78.9% | +14.3% |
| Summarization (ROUGE-L) | 47.2 | 38.5 | +8.7 |

### 4.2 Scaling Laws

Following Kaplan et al. (2020), we observe predictable performance scaling:
- **Parameters**: Log-linear improvement with model size
- **Data**: Performance scales as a power law of dataset size
- **Compute**: Optimal allocation follows N âˆ C^0.73

### 4.3 Efficiency Analysis

| Model | Parameters | Training Cost (GPU-hours) | Inference Latency (ms) |
|-------|-----------|--------------------------|----------------------|
| BERT-Base | 110M | 1,024 | 12 |
| GPT-3 | 175B | 355,000 | 85 |
| LLaMA-2-70B | 70B | 170,000 | 45 |
| Mistral-7B | 7B | 8,000 | 8 |

---

## 5. Discussion

### 5.1 Key Findings

1. **Transformer dominance**: Across all evaluated NLP tasks, transformer models achieve state-of-the-art results
2. **Scale matters**: Larger models generally perform better, but with diminishing returns beyond certain thresholds
3. **Efficiency gains**: Recent architectures (Mistral, Phi-3) show that smaller, well-trained models can match much larger predecessors
4. **Transfer learning**: Pre-trained transformers dramatically reduce downstream task data requirements

### 5.2 Limitations

- **Quadratic complexity**: Self-attention scales as O(nÂ²) with sequence length
- **Hallucination**: Models generate plausible but factually incorrect content
- **Environmental cost**: Training large models produces significant carbon emissions
- **Data contamination**: Benchmark scores may be inflated due to training data overlap

### 5.3 Ethical Considerations

The widespread deployment of transformer-based language models raises concerns about bias amplification, misinformation generation, and economic displacement of language workers.

---

## 6. Conclusion

Transformer architectures have fundamentally transformed NLP, delivering remarkable performance improvements across all major tasks. This survey identifies key trends: the shift toward decoder-only architectures, the importance of scaling laws, and the emergence of efficient alternatives to dense models. Future work should focus on reducing computational costs, improving factual grounding, and developing robust evaluation frameworks that capture real-world utility beyond benchmark performance.

---

## References

[1] Vaswani, A. et al. "Attention Is All You Need." NeurIPS, 2017.
[2] Devlin, J. et al. "BERT: Pre-training of Deep Bidirectional Transformers." NAACL, 2019.
[3] Brown, T. et al. "Language Models are Few-Shot Learners." NeurIPS, 2020.
[4] Raffel, C. et al. "Exploring the Limits of Transfer Learning with T5." JMLR, 2020.
[5] Touvron, H. et al. "LLaMA: Open and Efficient Foundation Language Models." arXiv, 2023.
[6] Jiang, A. et al. "Mistral 7B." arXiv, 2023.
[7] Kaplan, J. et al. "Scaling Laws for Neural Language Models." arXiv, 2020.
[8] Su, J. et al. "RoFormer: Enhanced Transformer with Rotary Position Embedding." arXiv, 2021.
`;

const reportMarkdown2 = `# Deep Learning Applications in Medical Imaging: A Systematic Review

*Generated by AI Research Engine â€” Healthcare / Biomedical Format*

*Date: 2026-02-10*

---

## Abstract

This systematic review examines the rapidly evolving field of deep learning applications in medical imaging. We analyzed 200+ peer-reviewed studies from 2020-2026, focusing on convolutional neural networks (CNNs), vision transformers (ViTs), and hybrid architectures deployed across radiology, pathology, and ophthalmology. Our findings demonstrate that AI-assisted diagnostic systems achieve sensitivity rates of 94-98% in detecting common pathologies, often matching or exceeding specialist-level performance. However, significant gaps persist in clinical deployment, regulatory frameworks, and health equity considerations.

**Keywords:** Deep Learning, Medical Imaging, CNN, Computer-Aided Diagnosis, Radiology, Clinical AI

---

## 1. Introduction

Medical imaging generates approximately 3.6 billion diagnostic procedures annually worldwide, creating an immense burden on healthcare systems. Deep learning models offer the potential to augment clinical decision-making, reduce diagnostic errors, and improve patient outcomes through faster and more consistent image interpretation.

The FDA has approved over 500 AI-enabled medical devices as of January 2026, with radiology accounting for approximately 75% of all approvals. Despite this regulatory progress, widespread clinical adoption remains limited, with only 30% of large academic medical centers routinely using AI-assisted imaging tools.

### 1.1 Objectives

1. Systematically review deep learning architectures used in medical imaging
2. Quantify diagnostic performance across imaging modalities
3. Identify barriers to clinical deployment
4. Recommend strategies for responsible AI integration in healthcare

---

## 2. Methods

### 2.1 Search Strategy

We searched PubMed, IEEE Xplore, and Google Scholar using terms: ("deep learning" OR "convolutional neural network" OR "vision transformer") AND ("medical imaging" OR "radiology" OR "pathology"). After screening 1,247 abstracts, 218 studies met our inclusion criteria.

### 2.2 Quality Assessment

Studies were evaluated using the QUADAS-2 framework for diagnostic accuracy studies, with additional criteria for AI-specific methodological rigor (data splitting, external validation, bias assessment).

---

## 3. Results

### 3.1 Architecture Performance by Modality

| Modality | Best Architecture | AUC-ROC | Sensitivity | Specificity |
|----------|------------------|---------|-------------|-------------|
| Chest X-ray | DenseNet-121 | 0.967 | 95.2% | 93.8% |
| Mammography | EfficientNet-B4 | 0.954 | 94.6% | 91.2% |
| CT (Lung) | 3D ResNet-50 | 0.978 | 96.8% | 95.1% |
| MRI (Brain) | ViT-Large | 0.971 | 95.9% | 94.3% |
| Retinal OCT | Inception-v3 | 0.963 | 94.1% | 92.7% |
| Pathology (WSI) | Swin Transformer | 0.982 | 97.2% | 96.0% |

### 3.2 Key Findings

- **Chest radiography** remains the most-studied application, with 67 studies demonstrating AI performance comparable to board-certified radiologists
- **Pathology** applications showed the highest diagnostic accuracy, particularly for cancer grading
- **Multi-modal fusion** models combining imaging with clinical data achieved 3-5% improvement over image-only models

### 3.3 Clinical Impact Metrics

| Metric | With AI Assistance | Without AI | Improvement |
|--------|-------------------|------------|-------------|
| Diagnostic Accuracy | 96.3% | 88.7% | +7.6% |
| Time per Case (min) | 3.2 | 7.8 | -59% |
| False Negative Rate | 2.1% | 8.3% | -74.7% |
| Inter-reader Agreement (Îº) | 0.92 | 0.78 | +17.9% |

---

## 4. Discussion

Deep learning has demonstrated transformative potential in medical imaging. However, several challenges remain:

1. **Generalizability**: Models trained on single-institution data often show 10-15% performance drops when deployed at external sites
2. **Explainability**: Only 23% of reviewed studies provided interpretable model outputs (e.g., attention maps, Grad-CAM)
3. **Bias**: Underrepresentation of diverse populations in training datasets may perpetuate health disparities
4. **Regulatory**: Current FDA pathways lack standardized evaluation criteria

---

## 5. Conclusion

Deep learning in medical imaging has reached a maturity level where clinical integration is both feasible and beneficial. Future research should prioritize multi-center validation studies, explainable AI methods, and equitable dataset curation to ensure safe and effective deployment across diverse healthcare settings.

---

## References

[1] Rajpurkar, P. et al. "AI in Health and Medicine." Nature Medicine, 2022.
[2] Esteva, A. et al. "Deep Learning-Enabled Medical Computer Vision." NPJ Digital Medicine, 2021.
[3] Zhou, S.K. et al. "A Review of Deep Learning in Medical Imaging." Proc. IEEE, 2021.
[4] Topol, E.J. "High-Performance Medicine." Nature Medicine, 2019.
[5] FDA. "Artificial Intelligence and Machine Learning in Medical Devices." 2026.
`;

const reportMarkdown3 = `# Quantum Computing for Cryptographic Security: Current State and Future Implications

*Generated by AI Research Engine â€” Computer Science / Cryptography Format*

*Date: 2026-01-28*

---

## Abstract

The advent of fault-tolerant quantum computers poses an existential threat to current public-key cryptographic systems. This research examines the timeline and implications of quantum attacks on RSA, ECC, and AES, while evaluating post-quantum cryptographic (PQC) alternatives recently standardized by NIST. Our analysis indicates that RSA-2048 could be broken by a quantum computer with approximately 4,000 logical qubits â€” a milestone potentially achievable within 5-10 years given current hardware trajectories. We provide a comprehensive risk assessment framework and migration roadmap for organizations transitioning to quantum-resistant cryptography.

**Keywords:** Quantum Computing, Post-Quantum Cryptography, RSA, Lattice-Based Cryptography, NIST PQC

---

## 1. Introduction

Shor's algorithm (1994) demonstrated that quantum computers can factorize large integers in polynomial time, fundamentally threatening RSA and elliptic curve cryptography (ECC). While current quantum computers lack the error correction capabilities needed for cryptographic attacks, rapid advances in superconducting qubits, trapped ions, and photonic systems suggest this barrier will be overcome within the next decade.

---

## 2. Quantum Threat Assessment

| Algorithm | Key Size | Classical Security | Quantum Security | Threat Level |
|-----------|----------|-------------------|------------------|------------|
| RSA-2048 | 2048-bit | 112 bits | 0 bits (Shor's) | ðŸ”´ Critical |
| ECC P-256 | 256-bit | 128 bits | 0 bits (Shor's) | ðŸ”´ Critical |
| AES-128 | 128-bit | 128 bits | 64 bits (Grover's) | ðŸŸ¡ Moderate |
| AES-256 | 256-bit | 256 bits | 128 bits (Grover's) | ðŸŸ¢ Safe |
| SHA-256 | 256-bit | 256 bits | 128 bits (Grover's) | ðŸŸ¢ Safe |

---

## 3. Post-Quantum Standards (NIST)

### 3.1 Selected Algorithms

| Algorithm | Type | Use Case | Key Size | Performance |
|-----------|------|----------|----------|-------------|
| ML-KEM (CRYSTALS-Kyber) | Lattice | Key Encapsulation | 1,568 B | Fast |
| ML-DSA (CRYSTALS-Dilithium) | Lattice | Digital Signatures | 2,420 B | Fast |
| SLH-DSA (SPHINCS+) | Hash-based | Digital Signatures | 64 B | Slower |
| FN-DSA (FALCON) | Lattice | Digital Signatures | 1,793 B | Medium |

---

## 4. Conclusion

Organizations must begin planning quantum-safe migration now. The "harvest now, decrypt later" threat means that sensitive data encrypted today with vulnerable algorithms could be retroactively decrypted once quantum computers mature.

---

## References

[1] Shor, P.W. "Algorithms for Quantum Computation." FOCS, 1994.
[2] NIST. "Post-Quantum Cryptography Standardization." 2024.
[3] Gidney, C. & EkerÃ¥, M. "How to Factor 2048-bit RSA in 8 Hours." Quantum, 2021.
`;

// â”€â”€â”€ LaTeX Source â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const latexSource1 = `\\documentclass[12pt, a4paper]{article}
\\usepackage[margin=1in]{geometry}
\\usepackage{times}
\\usepackage{graphicx}
\\usepackage{hyperref}
\\usepackage{amsmath}
\\usepackage{amssymb}
\\usepackage{fancyhdr}
\\usepackage{booktabs}
\\usepackage{longtable}
\\usepackage[utf8]{inputenc}

\\pagestyle{fancy}
\\fancyhf{}
\\rfoot{Page \\thepage}
\\lfoot{Generated by AI Research Engine}

\\title{Transformer Architectures for Natural Language Processing}
\\author{AI Research Engine}
\\date{\\today}

\\begin{document}
\\maketitle
\\tableofcontents
\\newpage

\\begin{abstract}
The transformer architecture has revolutionized natural language processing (NLP) since its introduction in 2017. This comprehensive survey examines the evolution, variants, and applications of transformer models across diverse NLP tasks. We analyze key architectural innovations including self-attention mechanisms, positional encodings, and scaling laws that have enabled models like BERT, GPT, and T5 to achieve state-of-the-art performance.
\\end{abstract}

\\section{Introduction}
Natural Language Processing has undergone a paradigm shift with the introduction of the transformer architecture by Vaswani et al. (2017). Unlike recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, transformers process all input tokens in parallel through self-attention mechanisms.

\\subsection{Research Objectives}
\\begin{enumerate}
\\item Provide a comprehensive taxonomy of transformer variants
\\item Analyze performance comparisons across standard NLP benchmarks
\\item Identify current limitations and open challenges
\\item Propose actionable future research directions
\\end{enumerate}

\\section{Background}
The self-attention mechanism computes:
\\begin{equation}
\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V
\\end{equation}

\\section{Results}
\\begin{table}[h]
\\centering
\\begin{tabular}{lccr}
\\toprule
Task & Transformer & Non-Transformer & Improvement \\\\
\\midrule
Text Classification & 96.8\\% & 89.2\\% & +7.6\\% \\\\
NER & 94.6\\% & 88.1\\% & +6.5\\% \\\\
Machine Translation & 46.4 & 35.8 & +10.6 \\\\
\\bottomrule
\\end{tabular}
\\caption{Performance comparison across NLP tasks}
\\end{table}

\\section{Conclusion}
Transformer architectures have fundamentally transformed NLP, delivering remarkable performance improvements across all major tasks.

\\begin{thebibliography}{9}
\bibitem{vaswani} Vaswani, A. et al. \`\`Attention Is All You Need.'' NeurIPS, 2017.
\bibitem{devlin} Devlin, J. et al. \`\`BERT: Pre-training of Deep Bidirectional Transformers.'' NAACL, 2019.
\bibitem{brown} Brown, T. et al. \`\`Language Models are Few-Shot Learners.'' NeurIPS, 2020.
\\end{thebibliography}

\\end{document}`;

// â”€â”€â”€ Mock Result JSONs (matching real pipeline output structure) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const MOCK_RESULT_JSON_1 = {
  status: 'completed',
  task: 'Transformer Architectures for Natural Language Processing',
  final_state: {
    task: 'Transformer Architectures for Natural Language Processing',
    topic_locked: true,
    selected_topic: 'Transformer Architectures for Natural Language Processing: A Comprehensive Survey',
    findings: {
      topic_discovery: {
        topic_suggestions: [
          { title: 'Transformer Architectures for NLP: A Comprehensive Survey', domain: 'computer_science', novelty_angle: 'survey', estimated_complexity: 'medium' },
        ],
      },
      topic_lock: { status: 'locked', title: 'Transformer Architectures for Natural Language Processing: A Comprehensive Survey' },
      domain_intelligence: {
        keywords: ['Transformer', 'Self-Attention', 'BERT', 'GPT', 'NLP', 'Language Models'],
        title: 'Comprehensive survey of transformer architectures in NLP',
        _meta_search_results: [
          { title: 'Attention Is All You Need', url: 'https://arxiv.org/abs/1706.03762', description: 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks...', source: 'arxiv' },
          { title: 'BERT: Pre-training of Deep Bidirectional Transformers', url: 'https://arxiv.org/abs/1810.04805', description: 'We introduce BERT, designed to pre-train deep bidirectional representations...', source: 'arxiv' },
        ],
      },
      historical_review: {
        raw_text: 'Transformer architectures have evolved rapidly since 2017...',
        _meta_history_sources: [
          { title: 'Language Models are Few-Shot Learners', url: 'https://arxiv.org/abs/2005.14165', description: 'GPT-3 demonstrates that scaling language models achieves strong few-shot performance...', published: '2020-05-28', authors: ['Tom Brown', 'et al.'], source: 'arxiv' },
          { title: 'Scaling Laws for Neural Language Models', url: 'https://arxiv.org/abs/2001.08361', description: 'Empirical scaling laws for language model performance on cross-entropy loss...', published: '2020-01-23', authors: ['Jared Kaplan', 'et al.'], source: 'arxiv' },
        ],
      },
      multi_stage_report: {
        markdown_report: reportMarkdown1,
        latex_source: latexSource1,
        word_count: 2850,
      },
      visualization: {
        response: {
          timeline_mermaid: timelineMermaid,
          methodology_mermaid: methodologyMermaid,
          data_chart_mermaid: dataChartMermaid,
          image_urls: [
            'https://images.unsplash.com/photo-1677442135143-6f18ab773e88?w=800',
            'https://images.unsplash.com/photo-1620712943543-bcc4688e7485?w=800',
            'https://images.unsplash.com/photo-1555255707-c07966088b7b?w=800',
            'https://images.unsplash.com/photo-1516110833967-0b5716ca1387?w=800',
          ],
        },
      },
      google_news: {
        response: {
          results: [
            { title: 'GPT-5 Expected to Revolutionize Enterprise AI', url: 'https://techcrunch.com/2026/gpt-5-enterprise', snippet: 'OpenAI is expected to release GPT-5 in mid-2026 with enhanced reasoning capabilities...' },
            { title: 'Google DeepMind Debuts Gemini 2.0 Ultra', url: 'https://blog.google/technology/ai/gemini-2-ultra', snippet: 'Gemini 2.0 Ultra achieves new SOTA on MMLU with 95.2% accuracy...' },
            { title: 'Open Source LLMs Close the Gap with Proprietary Models', url: 'https://venturebeat.com/2026/open-source-llm-gap', snippet: 'LLaMA-3 and Mistral-Large demonstrate competitive performance with GPT-4...' },
          ],
        },
      },
      literature_review: {
        response: {
          papers: [
            { title: 'Attention Is All You Need', url: 'https://arxiv.org/abs/1706.03762', abstract: 'We propose a new simple network architecture, the Transformer, based solely on attention mechanisms.', authors: ['Vaswani et al.'] },
            { title: 'BERT: Pre-training of Deep Bidirectional Transformers', url: 'https://arxiv.org/abs/1810.04805', abstract: 'We introduce BERT for pre-training deep bidirectional representations from unlabeled text.', authors: ['Devlin et al.'] },
            { title: 'LLaMA: Open and Efficient Foundation Language Models', url: 'https://arxiv.org/abs/2302.13971', abstract: 'We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters.', authors: ['Touvron et al.'] },
            { title: 'Mistral 7B', url: 'https://arxiv.org/abs/2310.06825', abstract: 'We introduce Mistral 7B which outperforms all 13B parameter models on all benchmarks.', authors: ['Jiang et al.'] },
          ],
        },
      },
    },
  },
};

export const MOCK_RESULT_JSON_2 = {
  status: 'completed',
  task: 'Deep Learning Applications in Medical Imaging',
  final_state: {
    task: 'Deep Learning Applications in Medical Imaging',
    topic_locked: true,
    selected_topic: 'Deep Learning Applications in Medical Imaging: A Systematic Review',
    findings: {
      topic_lock: { status: 'locked', title: 'Deep Learning Applications in Medical Imaging: A Systematic Review' },
      domain_intelligence: {
        keywords: ['Deep Learning', 'Medical Imaging', 'CNN', 'Radiology', 'Clinical AI'],
        title: 'Systematic review of deep learning in medical imaging',
      },
      multi_stage_report: { markdown_report: reportMarkdown2, word_count: 1800 },
      visualization: {
        response: {
          timeline_mermaid: timelineMermaid,
          methodology_mermaid: `flowchart TD
  A[Dataset Collection] --> B[Image Preprocessing]
  B --> C[Data Augmentation]
  C --> D[Model Training]
  D --> E{Validation}
  E -->|AUC > 0.95| F[External Testing]
  E -->|AUC < 0.95| G[Architecture Search]
  G --> D
  F --> H[Clinical Trial]
  H --> I[FDA Submission]`,
          data_chart_mermaid: `pie title Imaging Modalities Studied
  "Chest X-ray" : 30
  "CT Scans" : 25
  "MRI" : 20
  "Mammography" : 15
  "Retinal OCT" : 10`,
          image_urls: [
            'https://images.unsplash.com/photo-1559757175-5700dde675bc?w=800',
            'https://images.unsplash.com/photo-1576091160550-2173dba999ef?w=800',
            'https://images.unsplash.com/photo-1530497610245-b46e5f7e2299?w=800',
          ],
        },
      },
      google_news: {
        response: {
          results: [
            { title: 'AI Outperforms Radiologists in Breast Cancer Screening Study', url: 'https://nature.com/articles/ai-breast-cancer', snippet: 'A landmark multi-center trial shows AI systems detect 20% more cancers...' },
            { title: 'FDA Approves Record Number of AI Medical Devices in 2025', url: 'https://medtechdive.com/news/fda-ai-devices', snippet: 'Over 180 new AI/ML-enabled medical devices received FDA clearance...' },
          ],
        },
      },
      literature_review: {
        response: {
          papers: [
            { title: 'AI in Health and Medicine', url: 'https://nature.com/articles/s41591-021-01614-0', abstract: 'A comprehensive overview of AI applications across healthcare domains.', authors: ['Rajpurkar et al.'] },
            { title: 'Deep Learning-Enabled Medical Computer Vision', url: 'https://doi.org/10.1038/s41746-021-00412-3', abstract: 'Review of deep learning methods for medical image analysis tasks.', authors: ['Esteva et al.'] },
          ],
        },
      },
    },
  },
};

export const MOCK_RESULT_JSON_3 = {
  status: 'completed',
  task: 'Quantum Computing for Cryptographic Security',
  final_state: {
    task: 'Quantum Computing for Cryptographic Security',
    topic_locked: true,
    selected_topic: 'Quantum Computing for Cryptographic Security: Current State and Future Implications',
    findings: {
      topic_lock: { status: 'locked', title: 'Quantum Computing for Cryptographic Security: Current State and Future Implications' },
      domain_intelligence: {
        keywords: ['Quantum Computing', 'Post-Quantum Cryptography', 'RSA', 'Lattice-Based', 'NIST PQC'],
        title: 'Quantum threats to cryptographic security',
      },
      multi_stage_report: { markdown_report: reportMarkdown3, word_count: 1200 },
      visualization: {
        response: {
          timeline_mermaid: `gantt
  title Quantum Computing Milestones
  dateFormat YYYY
  section Hardware
  50 Qubits (IBM)        :done, 2019, 1y
  100 Qubits (Google)    :done, 2021, 1y
  1000+ Qubits (IBM)     :done, 2023, 1y
  Error Corrected Qubits :active, 2025, 2y
  section Standards
  NIST PQC Round 3       :done, 2022, 1y
  NIST PQC Final         :done, 2024, 1y
  Migration Deadline      :2030, 2y`,
          methodology_mermaid: `flowchart LR
  A[Classical Encryption] --> B{Quantum Computer?}
  B -->|No| C[Still Secure]
  B -->|Yes| D[Shor's Algorithm]
  D --> E[RSA Broken]
  D --> F[ECC Broken]
  A --> G[Post-Quantum Crypto]
  G --> H[Lattice-Based]
  G --> I[Hash-Based]
  G --> J[Code-Based]`,
          image_urls: [
            'https://images.unsplash.com/photo-1635070041078-e363dbe005cb?w=800',
            'https://images.unsplash.com/photo-1451187580459-43490279c0fa?w=800',
          ],
        },
      },
    },
  },
};

// â”€â”€â”€ Research Jobs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const MOCK_RESEARCHES = [
  {
    id: 1,
    title: 'Transformer Architectures for Natural Language Processing',
    task: 'Transformer Architectures for Natural Language Processing',
    status: 'completed',
    result_json: MOCK_RESULT_JSON_1,
    report_markdown: reportMarkdown1,
    latex_source: latexSource1,
    current_stage: 'completed',
    started_at: '2026-02-18T08:30:00Z',
    completed_at: '2026-02-18T08:58:42Z',
    created_at: '2026-02-18T08:29:15Z',
    updated_at: '2026-02-18T08:58:42Z',
    user_id: 1,
  },
  {
    id: 2,
    title: 'Deep Learning Applications in Medical Imaging',
    task: 'Deep Learning Applications in Medical Imaging',
    status: 'completed',
    result_json: MOCK_RESULT_JSON_2,
    report_markdown: reportMarkdown2,
    latex_source: null,
    current_stage: 'completed',
    started_at: '2026-02-10T14:15:00Z',
    completed_at: '2026-02-10T14:42:18Z',
    created_at: '2026-02-10T14:14:30Z',
    updated_at: '2026-02-10T14:42:18Z',
    user_id: 1,
  },
  {
    id: 3,
    title: 'Quantum Computing for Cryptographic Security',
    task: 'Quantum Computing for Cryptographic Security',
    status: 'completed',
    result_json: MOCK_RESULT_JSON_3,
    report_markdown: reportMarkdown3,
    latex_source: null,
    current_stage: 'completed',
    started_at: '2026-01-28T10:05:00Z',
    completed_at: '2026-01-28T10:31:55Z',
    created_at: '2026-01-28T10:04:20Z',
    updated_at: '2026-01-28T10:31:55Z',
    user_id: 1,
  },
  {
    id: 4,
    title: 'Reinforcement Learning in Autonomous Vehicles',
    task: 'Reinforcement Learning in Autonomous Vehicles',
    status: 'failed',
    result_json: null,
    report_markdown: null,
    latex_source: null,
    current_stage: 'literature_review',
    started_at: '2026-02-05T16:00:00Z',
    completed_at: '2026-02-05T16:12:30Z',
    created_at: '2026-02-05T15:59:00Z',
    updated_at: '2026-02-05T16:12:30Z',
    user_id: 1,
  },
];

// â”€â”€â”€ Mock Execution Events â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export function getMockEvents(researchId: number) {
  const base = [
    { event_id: 'evt-001', timestamp: '2026-02-18T08:30:00Z', stage: 'topic_discovery', severity: 'info' as const, category: 'agent', message: 'Starting topic discovery agent...', details: {} },
    { event_id: 'evt-002', timestamp: '2026-02-18T08:30:12Z', stage: 'topic_discovery', severity: 'success' as const, category: 'agent', message: 'Generated 3 topic suggestions based on initial query', details: { suggestions_count: 3 } },
    { event_id: 'evt-003', timestamp: '2026-02-18T08:30:25Z', stage: 'topic_lock', severity: 'info' as const, category: 'pipeline', message: 'Topic selected and locked: "Transformer Architectures for NLP"', details: {} },
    { event_id: 'evt-004', timestamp: '2026-02-18T08:31:00Z', stage: 'domain_intelligence', severity: 'info' as const, category: 'agent', message: 'Running domain intelligence agent â€” analyzing keywords and research landscape', details: {} },
    { event_id: 'evt-005', timestamp: '2026-02-18T08:32:15Z', stage: 'domain_intelligence', severity: 'success' as const, category: 'search', message: 'DuckDuckGo: Found 5 relevant results for "transformer NLP survey"', details: { provider: 'duckduckgo', count: 5 } },
    { event_id: 'evt-006', timestamp: '2026-02-18T08:32:45Z', stage: 'domain_intelligence', severity: 'success' as const, category: 'search', message: 'Google Scholar: Found 8 academic papers matching query', details: { provider: 'google_scholar', count: 8 } },
    { event_id: 'evt-007', timestamp: '2026-02-18T08:33:20Z', stage: 'domain_intelligence', severity: 'success' as const, category: 'agent', message: 'Domain analysis complete â€” identified 6 core keywords and 12 research gaps', details: { keywords: 6, gaps: 12 } },
    { event_id: 'evt-008', timestamp: '2026-02-18T08:34:00Z', stage: 'literature_review', severity: 'info' as const, category: 'agent', message: 'Starting literature review â€” searching Arxiv, PubMed, OpenAlex', details: {} },
    { event_id: 'evt-009', timestamp: '2026-02-18T08:35:30Z', stage: 'literature_review', severity: 'success' as const, category: 'search', message: 'Arxiv: Retrieved 15 papers on transformer architectures', details: { count: 15 } },
    { event_id: 'evt-010', timestamp: '2026-02-18T08:36:45Z', stage: 'literature_review', severity: 'success' as const, category: 'search', message: 'OpenAlex: Found 12 additional papers with citation data', details: { count: 12 } },
    { event_id: 'evt-011', timestamp: '2026-02-18T08:38:00Z', stage: 'literature_review', severity: 'warn' as const, category: 'search', message: 'PubMed: Rate limited â€” retrying with exponential backoff', details: { retry: 1 } },
    { event_id: 'evt-012', timestamp: '2026-02-18T08:39:00Z', stage: 'literature_review', severity: 'success' as const, category: 'search', message: 'PubMed: Retrieved 8 relevant biomedical NLP papers', details: { count: 8 } },
    { event_id: 'evt-013', timestamp: '2026-02-18T08:40:00Z', stage: 'web_scraper', severity: 'info' as const, category: 'agent', message: 'Starting web scraper â€” extracting content from 10 URLs', details: { urls: 10 } },
    { event_id: 'evt-014', timestamp: '2026-02-18T08:42:30Z', stage: 'web_scraper', severity: 'success' as const, category: 'scraping', message: 'Successfully scraped 8/10 URLs â€” extracted 45,000 tokens of content', details: { success: 8, failed: 2, tokens: 45000 } },
    { event_id: 'evt-015', timestamp: '2026-02-18T08:42:45Z', stage: 'web_scraper', severity: 'warn' as const, category: 'scraping', message: 'Failed to scrape 2 URLs: access denied (403)', details: { failed_urls: 2 } },
    { event_id: 'evt-016', timestamp: '2026-02-18T08:43:30Z', stage: 'google_news', severity: 'info' as const, category: 'agent', message: 'Fetching latest news on transformer models and LLMs', details: {} },
    { event_id: 'evt-017', timestamp: '2026-02-18T08:44:15Z', stage: 'google_news', severity: 'success' as const, category: 'search', message: 'Found 6 recent news articles from the past 30 days', details: { count: 6 } },
    { event_id: 'evt-018', timestamp: '2026-02-18T08:45:00Z', stage: 'novelty_check', severity: 'info' as const, category: 'agent', message: 'Running novelty assessment â€” comparing against existing surveys', details: {} },
    { event_id: 'evt-019', timestamp: '2026-02-18T08:46:30Z', stage: 'novelty_check', severity: 'success' as const, category: 'agent', message: 'Novelty score: 7.2/10 â€” unique angle identified on efficiency-accuracy tradeoff', details: { score: 7.2 } },
    { event_id: 'evt-020', timestamp: '2026-02-18T08:47:00Z', stage: 'scoring', severity: 'info' as const, category: 'agent', message: 'Evaluating research quality metrics...', details: {} },
    { event_id: 'evt-021', timestamp: '2026-02-18T08:48:00Z', stage: 'scoring', severity: 'success' as const, category: 'agent', message: 'Quality score: 8.5/10 â€” strong methodology and comprehensive coverage', details: { score: 8.5 } },
    { event_id: 'evt-022', timestamp: '2026-02-18T08:49:00Z', stage: 'synthesis', severity: 'info' as const, category: 'agent', message: 'Starting report synthesis â€” generating comprehensive research paper', details: {} },
    { event_id: 'evt-023', timestamp: '2026-02-18T08:52:00Z', stage: 'synthesis', severity: 'info' as const, category: 'llm', message: 'LLM generating report... (2,850 words so far)', details: { words: 2850 } },
    { event_id: 'evt-024', timestamp: '2026-02-18T08:54:00Z', stage: 'synthesis', severity: 'success' as const, category: 'agent', message: 'Report synthesis complete â€” 2,850 words with 8 references', details: { words: 2850, refs: 8 } },
    { event_id: 'evt-025', timestamp: '2026-02-18T08:55:00Z', stage: 'visualization', severity: 'info' as const, category: 'agent', message: 'Generating visualizations â€” timeline, methodology flowchart, data chart', details: {} },
    { event_id: 'evt-026', timestamp: '2026-02-18T08:56:30Z', stage: 'visualization', severity: 'success' as const, category: 'agent', message: 'Generated 3 Mermaid diagrams and found 4 relevant images', details: { diagrams: 3, images: 4 } },
    { event_id: 'evt-027', timestamp: '2026-02-18T08:57:00Z', stage: 'latex_generation', severity: 'info' as const, category: 'agent', message: 'Converting report to LaTeX format...', details: {} },
    { event_id: 'evt-028', timestamp: '2026-02-18T08:58:00Z', stage: 'latex_generation', severity: 'success' as const, category: 'agent', message: 'LaTeX source generated â€” ready for compilation', details: {} },
    { event_id: 'evt-029', timestamp: '2026-02-18T08:58:30Z', stage: 'completed', severity: 'success' as const, category: 'pipeline', message: 'Research pipeline completed successfully in 28m 30s', details: { duration_seconds: 1710 } },
  ];
  return base.map(e => ({ ...e, type: 'event' }));
}

// â”€â”€â”€ Mock Data Sources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export function getMockSources(researchId: number) {
  return [
    { source_type: 'arxiv', domain: 'arxiv.org', url: 'https://arxiv.org/abs/1706.03762', status: 'success', items_found: 1, title: 'Attention Is All You Need', description: 'The Transformer architecture based on self-attention mechanisms', favicon: 'https://arxiv.org/favicon.ico', published_date: '2017-06-12' },
    { source_type: 'arxiv', domain: 'arxiv.org', url: 'https://arxiv.org/abs/1810.04805', status: 'success', items_found: 1, title: 'BERT: Pre-training of Deep Bidirectional Transformers', description: 'Bidirectional pre-training for language understanding', favicon: 'https://arxiv.org/favicon.ico', published_date: '2019-05-24' },
    { source_type: 'arxiv', domain: 'arxiv.org', url: 'https://arxiv.org/abs/2005.14165', status: 'success', items_found: 1, title: 'Language Models are Few-Shot Learners (GPT-3)', description: 'Scaling language models achieves strong few-shot performance', favicon: 'https://arxiv.org/favicon.ico', published_date: '2020-05-28' },
    { source_type: 'arxiv', domain: 'arxiv.org', url: 'https://arxiv.org/abs/2302.13971', status: 'success', items_found: 1, title: 'LLaMA: Open Foundation Language Models', description: 'Collection of foundation models from 7B to 65B parameters', favicon: 'https://arxiv.org/favicon.ico', published_date: '2023-02-27' },
    { source_type: 'arxiv', domain: 'arxiv.org', url: 'https://arxiv.org/abs/2310.06825', status: 'success', items_found: 1, title: 'Mistral 7B', description: 'A 7B model outperforming all 13B models on benchmarks', favicon: 'https://arxiv.org/favicon.ico', published_date: '2023-10-10' },
    { source_type: 'web', domain: 'en.wikipedia.org', url: 'https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)', status: 'success', items_found: 3, title: 'Transformer (deep learning architecture)', description: 'Wikipedia comprehensive article on transformer architecture', favicon: 'https://en.wikipedia.org/favicon.ico' },
    { source_type: 'web', domain: 'huggingface.co', url: 'https://huggingface.co/docs/transformers', status: 'success', items_found: 5, title: 'Hugging Face Transformers Documentation', description: 'State-of-the-art NLP library with pre-trained models', favicon: 'https://huggingface.co/favicon.ico' },
    { source_type: 'news', domain: 'techcrunch.com', url: 'https://techcrunch.com/2026/gpt-5-enterprise', status: 'success', items_found: 1, title: 'GPT-5 Expected to Revolutionize Enterprise AI', description: 'OpenAI is expected to release GPT-5 in mid-2026', favicon: 'https://techcrunch.com/favicon.ico', published_date: '2026-02-15' },
    { source_type: 'news', domain: 'blog.google', url: 'https://blog.google/technology/ai/gemini-2-ultra', status: 'success', items_found: 1, title: 'Google DeepMind Debuts Gemini 2.0 Ultra', description: 'Gemini 2.0 Ultra achieves new SOTA on MMLU with 95.2%', favicon: 'https://blog.google/favicon.ico', published_date: '2026-02-10' },
    { source_type: 'web', domain: 'paperswithcode.com', url: 'https://paperswithcode.com/method/transformer', status: 'success', items_found: 8, title: 'Transformer - Papers With Code', description: 'Latest transformer research with benchmark results' },
    { source_type: 'kaggle', domain: 'kaggle.com', url: 'https://kaggle.com/datasets/transformers-benchmark', status: 'success', items_found: 2, title: 'NLP Benchmarks Dataset', description: 'Comprehensive NLP evaluation datasets for transformer models' },
    { source_type: 'github', domain: 'github.com', url: 'https://github.com/huggingface/transformers', status: 'success', items_found: 1, title: 'huggingface/transformers', description: 'State-of-the-art ML for PyTorch, TensorFlow, and JAX (140k+ stars)' },
  ];
}

// â”€â”€â”€ Mock Chat History â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const MOCK_CHAT_HISTORY = [
  { role: 'user' as const, message: 'What are the main findings of this research?', created_at: '2026-02-18T09:00:00Z' },
  { role: 'assistant' as const, message: `Based on the research paper "Transformer Architectures for Natural Language Processing: A Comprehensive Survey", here are the **main findings**:

## Key Results

1. **Transformer Dominance**: Across all evaluated NLP tasks, transformer models achieve state-of-the-art results, outperforming non-transformer baselines by 7-15%.

2. **Scaling Laws**: Larger models generally perform better, following predictable power-law relationships between model size, dataset size, compute, and performance.

3. **Efficiency Breakthrough**: Recent models like Mistral-7B and Phi-3 demonstrate that smaller, well-trained models can match much larger predecessors (e.g., GPT-3).

4. **Architecture Trends**: The field is shifting toward decoder-only architectures (GPT-style) and Mixture-of-Experts (MoE) for improved efficiency.

| Task | Best Score | Improvement over non-Transformer |
|------|-----------|------|
| Text Classification | 96.8% | +7.6% |
| NER | 94.6% | +6.5% |
| Machine Translation | 46.4 BLEU | +10.6 |
| QA (F1) | 93.2% | +14.3% |

Would you like me to elaborate on any specific finding?`, created_at: '2026-02-18T09:00:15Z' },
  { role: 'user' as const, message: 'What are the limitations mentioned?', created_at: '2026-02-18T09:02:00Z' },
  { role: 'assistant' as const, message: `The paper identifies several important **limitations** of transformer architectures:

### Technical Limitations
- **Quadratic Complexity**: Self-attention scales as O(nÂ²) with sequence length, making long-document processing expensive
- **Hallucination**: Models generate plausible but factually incorrect content, a critical issue for production deployment
- **Data Contamination**: Benchmark scores may be inflated due to overlap between training data and evaluation sets

### Practical Challenges
- **Environmental Cost**: Training GPT-3 produced ~500 tonnes of COâ‚‚ equivalent
- **Compute Requirements**: State-of-the-art models require massive GPU clusters (355K GPU-hours for GPT-3)
- **Diminishing Returns**: Performance improvements plateau beyond certain model sizes

### Ethical Concerns
- Bias amplification from training data
- Potential for misinformation generation
- Economic displacement of language workers

These limitations suggest that future research should prioritize **efficient architectures** (linear attention, sparse transformers) and **robust evaluation frameworks** over pure scaling.`, created_at: '2026-02-18T09:02:20Z' },
];

// â”€â”€â”€ Mock Memories â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const MOCK_MEMORIES = [
  { id: 1, content: 'Transformer models use self-attention mechanism with O(nÂ²) complexity', source: 'research' as const, source_id: 1, metadata: { research_title: 'Transformer Architectures for NLP' }, created_at: '2026-02-18T09:00:00Z' },
  { id: 2, content: 'BERT introduced bidirectional pre-training, achieving state-of-the-art on 11 NLP benchmarks', source: 'research' as const, source_id: 1, metadata: {}, created_at: '2026-02-18T09:01:00Z' },
  { id: 3, content: 'Deep learning in medical imaging achieves 94-98% sensitivity in detecting common pathologies', source: 'research' as const, source_id: 2, metadata: {}, created_at: '2026-02-10T15:00:00Z' },
  { id: 4, content: 'Post-quantum cryptography standards: ML-KEM (Kyber), ML-DSA (Dilithium), SLH-DSA (SPHINCS+)', source: 'research' as const, source_id: 3, metadata: {}, created_at: '2026-01-28T11:00:00Z' },
  { id: 5, content: 'Research methodology: Always start with systematic literature review before formulating hypotheses', source: 'manual' as const, metadata: {}, created_at: '2026-01-20T10:00:00Z' },
];

// â”€â”€â”€ Mock Usage Stats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const MOCK_USAGE_STATS = {
  totalTokens: 285400,
  cost: 4.28,
  history: [
    { date: '2026-02-18', tokens: 95000, provider: 'gemini' as const },
    { date: '2026-02-10', tokens: 72000, provider: 'gemini' as const },
    { date: '2026-01-28', tokens: 48000, provider: 'gemini' as const },
    { date: '2026-02-05', tokens: 35400, provider: 'gemini' as const },
    { date: '2026-01-20', tokens: 35000, provider: 'gemini' as const },
  ],
};

// â”€â”€â”€ Mock LLM Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const MOCK_LLM_STATUS = {
  mode: 'ONLINE' as const,
  provider: {
    provider: 'gemini',
    model: 'gemini-2.0-flash',
    available: true,
    total_keys: 2,
    active_key_index: 0,
  },
  config: {
    model_reasoning: 'gemini-2.0-flash-thinking-exp',
    model_writing: 'gemini-2.0-flash',
    model_coding: 'gemini-2.0-flash',
    model_critical: 'gemini-2.0-flash',
    max_tokens: 8192,
  },
};
