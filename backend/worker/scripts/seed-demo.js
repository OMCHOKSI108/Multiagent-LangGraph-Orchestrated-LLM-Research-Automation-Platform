/**
 * Seed Demo Account
 * 
 * Creates the demo user (devang@gmail.com / OMchoksi@30) with pre-populated
 * research data so the frontend developer can see every UI component in action.
 * 
 * Usage:
 *   cd backend
 *   node scripts/seed-demo.js
 * 
 * Requires: DB_USER, DB_HOST, DB_NAME, DB_PASSWORD, DB_PORT env vars
 *           (loaded from .env automatically)
 */

const dotenv = require('dotenv');
dotenv.config();

const { Pool } = require('pg');
const bcrypt = require('bcrypt');
const crypto = require('crypto');

const pool = new Pool({
  user: process.env.DB_USER,
  host: process.env.DB_HOST,
  database: process.env.DB_NAME,
  password: process.env.DB_PASSWORD,
  port: process.env.DB_PORT,
});

const DEMO_EMAIL = 'devang@gmail.com';
const DEMO_PASSWORD = 'OMchoksi@30';
const DEMO_USERNAME = 'Devang Dhandhukiya';

// ─── Mock Result JSONs ───────────────────────────────────────────────────────

const reportMarkdown1 = `# Transformer Architectures for Natural Language Processing: A Comprehensive Survey

*Generated by AI Research Engine — Computer Science / NLP Format*
*Date: 2026-02-18*

---

## Abstract

The transformer architecture has revolutionized natural language processing (NLP) since its introduction in 2017. This comprehensive survey examines the evolution, variants, and applications of transformer models across diverse NLP tasks. We analyze key architectural innovations including self-attention mechanisms, positional encodings, and scaling laws that have enabled models like BERT, GPT, and T5 to achieve state-of-the-art performance.

**Keywords:** Transformer, Self-Attention, BERT, GPT, NLP, Deep Learning, Language Models

---

## 1. Introduction

Natural Language Processing has undergone a paradigm shift with the introduction of the transformer architecture by Vaswani et al. (2017). Unlike recurrent neural networks (RNNs), transformers process all input tokens in parallel through self-attention mechanisms, enabling substantially faster training on modern hardware.

### 1.1 Research Objectives
1. Provide a comprehensive taxonomy of transformer variants
2. Analyze performance comparisons across standard NLP benchmarks
3. Identify current limitations and open challenges
4. Propose actionable future research directions

---

## 2. Background

| Model | Year | Parameters | GLUE Score |
|-------|------|-----------|------------|
| ELMo | 2018 | 94M | 68.7 |
| BERT-Base | 2019 | 110M | 79.6 |
| GPT-2 | 2019 | 1.5B | 72.5 |
| T5-Large | 2020 | 770M | 88.5 |
| GPT-4 | 2023 | ~1.8T | 92.1 |
| Gemini Ultra | 2024 | ~1.5T | 93.4 |

---

## 3. Results

| Task | Best Transformer | Best Non-Transformer | Improvement |
|------|-----------------|---------------------|-------------|
| Text Classification | 96.8% | 89.2% | +7.6% |
| Named Entity Recognition | 94.6% | 88.1% | +6.5% |
| Machine Translation (BLEU) | 46.4 | 35.8 | +10.6 |
| Question Answering (F1) | 93.2% | 78.9% | +14.3% |

---

## 4. Conclusion

Transformer architectures have fundamentally transformed NLP, delivering remarkable performance improvements across all major tasks.

## References

[1] Vaswani, A. et al. "Attention Is All You Need." NeurIPS, 2017.
[2] Devlin, J. et al. "BERT." NAACL, 2019.
[3] Brown, T. et al. "Language Models are Few-Shot Learners." NeurIPS, 2020.
`;

const latexSource1 = `\\documentclass[12pt, a4paper]{article}
\\usepackage[margin=1in]{geometry}
\\usepackage{times}
\\usepackage{graphicx}
\\usepackage{hyperref}
\\usepackage{booktabs}
\\usepackage[utf8]{inputenc}

\\title{Transformer Architectures for NLP}
\\author{AI Research Engine}
\\date{\\today}

\\begin{document}
\\maketitle

\\begin{abstract}
The transformer architecture has revolutionized NLP since 2017.
\\end{abstract}

\\section{Introduction}
Natural Language Processing has undergone a paradigm shift.

\\section{Results}
\\begin{table}[h]
\\centering
\\begin{tabular}{lccr}
\\toprule
Task & Transformer & Non-Transformer & Improvement \\\\
\\midrule
Text Classification & 96.8\\% & 89.2\\% & +7.6\\% \\\\
NER & 94.6\\% & 88.1\\% & +6.5\\% \\\\
\\bottomrule
\\end{tabular}
\\end{table}

\\end{document}`;

const resultJson1 = {
  status: 'completed',
  task: 'Transformer Architectures for Natural Language Processing',
  final_state: {
    task: 'Transformer Architectures for Natural Language Processing',
    topic_locked: true,
    selected_topic: 'Transformer Architectures for NLP: A Comprehensive Survey',
    findings: {
      topic_lock: { status: 'locked', title: 'Transformer Architectures for NLP: A Comprehensive Survey' },
      domain_intelligence: {
        keywords: ['Transformer', 'Self-Attention', 'BERT', 'GPT', 'NLP'],
        title: 'Comprehensive survey of transformer architectures',
        _meta_search_results: [
          { title: 'Attention Is All You Need', url: 'https://arxiv.org/abs/1706.03762', description: 'We propose the Transformer architecture...', source: 'arxiv' },
        ],
      },
      multi_stage_report: { markdown_report: reportMarkdown1, latex_source: latexSource1, word_count: 1500 },
      visualization: {
        response: {
          timeline_mermaid: `gantt\n  title Research Timeline\n  dateFormat YYYY-MM-DD\n  section Discovery\n  Literature Search :done, d1, 2026-01-15, 3d`,
          methodology_mermaid: `flowchart TD\n  A[Research Question] --> B[Literature Review]\n  B --> C[Data Collection]\n  C --> D[Analysis]`,
          data_chart_mermaid: `pie title Sources\n  "Arxiv" : 35\n  "PubMed" : 25\n  "Google Scholar" : 20`,
          image_urls: [
            'https://images.unsplash.com/photo-1677442135143-6f18ab773e88?w=800',
            'https://images.unsplash.com/photo-1620712943543-bcc4688e7485?w=800',
          ],
        },
      },
      google_news: {
        response: {
          results: [
            { title: 'GPT-5 Expected to Revolutionize Enterprise AI', url: 'https://techcrunch.com/2026/gpt-5', snippet: 'OpenAI expected to release GPT-5...' },
          ],
        },
      },
      literature_review: {
        response: {
          papers: [
            { title: 'Attention Is All You Need', url: 'https://arxiv.org/abs/1706.03762', abstract: 'We propose the Transformer.', authors: ['Vaswani et al.'] },
            { title: 'BERT', url: 'https://arxiv.org/abs/1810.04805', abstract: 'Bidirectional pre-training.', authors: ['Devlin et al.'] },
          ],
        },
      },
    },
  },
};

const reportMarkdown2 = `# Deep Learning in Medical Imaging: A Systematic Review

*Generated by AI Research Engine*
*Date: 2026-02-10*

## Abstract
This systematic review examines deep learning applications in medical imaging. AI-assisted systems achieve 94-98% sensitivity in detecting common pathologies.

## Results

| Modality | Best Architecture | AUC-ROC | Sensitivity |
|----------|------------------|---------|-------------|
| Chest X-ray | DenseNet-121 | 0.967 | 95.2% |
| CT (Lung) | 3D ResNet-50 | 0.978 | 96.8% |
| MRI (Brain) | ViT-Large | 0.971 | 95.9% |

## Conclusion
Deep learning in medical imaging has reached a maturity level where clinical integration is feasible.
`;

const resultJson2 = {
  status: 'completed',
  task: 'Deep Learning Applications in Medical Imaging',
  final_state: {
    task: 'Deep Learning Applications in Medical Imaging',
    topic_locked: true,
    findings: {
      topic_lock: { status: 'locked', title: 'Deep Learning in Medical Imaging' },
      multi_stage_report: { markdown_report: reportMarkdown2, word_count: 800 },
      visualization: {
        response: {
          timeline_mermaid: `gantt\n  title Study Timeline\n  dateFormat YYYY-MM-DD\n  section Phase I\n  Data Collection :done, 2026-01-01, 10d`,
          image_urls: ['https://images.unsplash.com/photo-1559757175-5700dde675bc?w=800'],
        },
      },
    },
  },
};

const reportMarkdown3 = `# Quantum Computing for Cryptographic Security

*Generated by AI Research Engine*
*Date: 2026-01-28*

## Abstract
Quantum computers pose an existential threat to current public-key cryptography. RSA-2048 could be broken with ~4,000 logical qubits.

## Quantum Threat Assessment

| Algorithm | Classical Security | Quantum Security | Threat |
|-----------|-------------------|------------------|--------|
| RSA-2048 | 112 bits | 0 bits | Critical |
| AES-256 | 256 bits | 128 bits | Safe |

## Conclusion
Organizations must plan quantum-safe migration now.
`;

const resultJson3 = {
  status: 'completed',
  task: 'Quantum Computing for Cryptographic Security',
  final_state: {
    task: 'Quantum Computing for Cryptographic Security',
    topic_locked: true,
    findings: {
      topic_lock: { status: 'locked', title: 'Quantum Computing for Cryptographic Security' },
      multi_stage_report: { markdown_report: reportMarkdown3, word_count: 600 },
      visualization: {
        response: {
          methodology_mermaid: `flowchart LR\n  A[Classical] --> B{Quantum?}\n  B -->|Yes| C[Broken]\n  B -->|No| D[Safe]`,
          image_urls: ['https://images.unsplash.com/photo-1635070041078-e363dbe005cb?w=800'],
        },
      },
    },
  },
};

// ─── Mock Events ─────────────────────────────────────────────────────────────

function getEvents(researchId) {
  return [
    { event_id: `evt-${researchId}-001`, stage: 'topic_discovery', severity: 'info', category: 'agent', message: 'Starting topic discovery agent...', details: {} },
    { event_id: `evt-${researchId}-002`, stage: 'topic_discovery', severity: 'success', category: 'agent', message: 'Generated topic suggestions', details: { count: 3 } },
    { event_id: `evt-${researchId}-003`, stage: 'topic_lock', severity: 'info', category: 'pipeline', message: 'Topic selected and locked', details: {} },
    { event_id: `evt-${researchId}-004`, stage: 'domain_intelligence', severity: 'info', category: 'agent', message: 'Running domain intelligence agent', details: {} },
    { event_id: `evt-${researchId}-005`, stage: 'domain_intelligence', severity: 'success', category: 'search', message: 'DuckDuckGo: Found 5 results', details: { provider: 'duckduckgo', count: 5 } },
    { event_id: `evt-${researchId}-006`, stage: 'domain_intelligence', severity: 'success', category: 'search', message: 'Google Scholar: Found 8 papers', details: { provider: 'google_scholar', count: 8 } },
    { event_id: `evt-${researchId}-007`, stage: 'literature_review', severity: 'info', category: 'agent', message: 'Starting literature review', details: {} },
    { event_id: `evt-${researchId}-008`, stage: 'literature_review', severity: 'success', category: 'search', message: 'Arxiv: Retrieved 15 papers', details: { count: 15 } },
    { event_id: `evt-${researchId}-009`, stage: 'literature_review', severity: 'warn', category: 'search', message: 'PubMed: Rate limited — retrying', details: { retry: 1 } },
    { event_id: `evt-${researchId}-010`, stage: 'web_scraper', severity: 'info', category: 'agent', message: 'Scraping 10 URLs', details: { urls: 10 } },
    { event_id: `evt-${researchId}-011`, stage: 'web_scraper', severity: 'success', category: 'scraping', message: 'Successfully scraped 8/10 URLs', details: { success: 8, failed: 2 } },
    { event_id: `evt-${researchId}-012`, stage: 'scoring', severity: 'success', category: 'agent', message: 'Quality score: 8.5/10', details: { score: 8.5 } },
    { event_id: `evt-${researchId}-013`, stage: 'synthesis', severity: 'info', category: 'agent', message: 'Generating comprehensive report', details: {} },
    { event_id: `evt-${researchId}-014`, stage: 'synthesis', severity: 'success', category: 'agent', message: 'Report synthesis complete', details: { words: 2500 } },
    { event_id: `evt-${researchId}-015`, stage: 'visualization', severity: 'success', category: 'agent', message: 'Generated diagrams and images', details: { diagrams: 3, images: 4 } },
    { event_id: `evt-${researchId}-016`, stage: 'completed', severity: 'success', category: 'pipeline', message: 'Pipeline completed successfully', details: { duration_seconds: 1710 } },
  ];
}

function getSources(researchId) {
  return [
    { source_type: 'arxiv', domain: 'arxiv.org', url: 'https://arxiv.org/abs/1706.03762', status: 'success', items_found: 1, title: 'Attention Is All You Need', description: 'The Transformer architecture' },
    { source_type: 'arxiv', domain: 'arxiv.org', url: 'https://arxiv.org/abs/1810.04805', status: 'success', items_found: 1, title: 'BERT', description: 'Bidirectional pre-training' },
    { source_type: 'arxiv', domain: 'arxiv.org', url: 'https://arxiv.org/abs/2005.14165', status: 'success', items_found: 1, title: 'GPT-3', description: 'Few-shot learning at scale' },
    { source_type: 'web', domain: 'en.wikipedia.org', url: 'https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)', status: 'success', items_found: 3, title: 'Transformer (Wikipedia)' },
    { source_type: 'web', domain: 'huggingface.co', url: 'https://huggingface.co/docs/transformers', status: 'success', items_found: 5, title: 'HuggingFace Transformers' },
    { source_type: 'news', domain: 'techcrunch.com', url: 'https://techcrunch.com/2026/gpt-5', status: 'success', items_found: 1, title: 'GPT-5 Enterprise AI' },
    { source_type: 'kaggle', domain: 'kaggle.com', url: 'https://kaggle.com/datasets/nlp-benchmarks', status: 'success', items_found: 2, title: 'NLP Benchmarks Dataset' },
    { source_type: 'github', domain: 'github.com', url: 'https://github.com/huggingface/transformers', status: 'success', items_found: 1, title: 'huggingface/transformers' },
  ];
}

// ─── Main seed function ──────────────────────────────────────────────────────

async function seed() {
  const client = await pool.connect();
  try {
    await client.query('BEGIN');

    // 1. Create or get demo user
    let userId;
    const existingUser = await client.query('SELECT id FROM users WHERE email = $1', [DEMO_EMAIL]);
    if (existingUser.rows.length > 0) {
      userId = existingUser.rows[0].id;
      console.log(`✓ Demo user already exists (id=${userId})`);
    } else {
      const salt = await bcrypt.genSalt(10);
      const hash = await bcrypt.hash(DEMO_PASSWORD, salt);
      const userResult = await client.query(
        'INSERT INTO users (username, email, password_hash) VALUES ($1, $2, $3) RETURNING id',
        [DEMO_USERNAME, DEMO_EMAIL, hash]
      );
      userId = userResult.rows[0].id;
      console.log(`✓ Created demo user (id=${userId})`);
    }

    // 2. Ensure API key exists
    const existingKey = await client.query('SELECT id FROM api_keys WHERE user_id = $1', [userId]);
    if (existingKey.rows.length === 0) {
      const apiKey = crypto.randomBytes(32).toString('hex');
      await client.query(
        'INSERT INTO api_keys (user_id, key_value, name, is_active) VALUES ($1, $2, $3, TRUE)',
        [userId, apiKey, 'Demo Key']
      );
      console.log(`✓ Created API key`);
    }

    // 3. Delete existing demo research (clean slate)
    await client.query('DELETE FROM research_logs WHERE user_id = $1', [userId]);
    console.log(`✓ Cleaned existing demo research data`);

    // 4. Insert mock researches
    const researches = [
      {
        title: 'Transformer Architectures for Natural Language Processing',
        task: 'Transformer Architectures for Natural Language Processing',
        status: 'completed',
        result_json: resultJson1,
        report_markdown: reportMarkdown1,
        latex_source: latexSource1,
        current_stage: 'completed',
        started_at: '2026-02-18T08:30:00Z',
        completed_at: '2026-02-18T08:58:42Z',
        created_at: '2026-02-18T08:29:15Z',
      },
      {
        title: 'Deep Learning Applications in Medical Imaging',
        task: 'Deep Learning Applications in Medical Imaging',
        status: 'completed',
        result_json: resultJson2,
        report_markdown: reportMarkdown2,
        latex_source: null,
        current_stage: 'completed',
        started_at: '2026-02-10T14:15:00Z',
        completed_at: '2026-02-10T14:42:18Z',
        created_at: '2026-02-10T14:14:30Z',
      },
      {
        title: 'Quantum Computing for Cryptographic Security',
        task: 'Quantum Computing for Cryptographic Security',
        status: 'completed',
        result_json: resultJson3,
        report_markdown: reportMarkdown3,
        latex_source: null,
        current_stage: 'completed',
        started_at: '2026-01-28T10:05:00Z',
        completed_at: '2026-01-28T10:31:55Z',
        created_at: '2026-01-28T10:04:20Z',
      },
      {
        title: 'Reinforcement Learning in Autonomous Vehicles',
        task: 'Reinforcement Learning in Autonomous Vehicles',
        status: 'failed',
        result_json: null,
        report_markdown: null,
        latex_source: null,
        current_stage: 'literature_review',
        started_at: '2026-02-05T16:00:00Z',
        completed_at: '2026-02-05T16:12:30Z',
        created_at: '2026-02-05T15:59:00Z',
      },
    ];

    const researchIds = [];
    for (const r of researches) {
      const res = await client.query(
        `INSERT INTO research_logs 
         (user_id, title, task, status, result_json, report_markdown, latex_source, current_stage, started_at, completed_at, created_at, updated_at)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, NOW())
         RETURNING id`,
        [userId, r.title, r.task, r.status, r.result_json ? JSON.stringify(r.result_json) : null,
         r.report_markdown, r.latex_source, r.current_stage, r.started_at, r.completed_at, r.created_at]
      );
      researchIds.push(res.rows[0].id);
      console.log(`✓ Created research: "${r.title}" (id=${res.rows[0].id})`);
    }

    // 5. Insert mock execution events for the first (most complete) research
    const mainResearchId = researchIds[0];
    const events = getEvents(mainResearchId);
    let baseTime = new Date('2026-02-18T08:30:00Z');
    for (const evt of events) {
      baseTime = new Date(baseTime.getTime() + 90_000); // 90s apart
      await client.query(
        `INSERT INTO execution_events (research_id, event_id, stage, severity, category, message, details, created_at)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
         ON CONFLICT (event_id) DO NOTHING`,
        [mainResearchId, evt.event_id, evt.stage, evt.severity, evt.category, evt.message,
         JSON.stringify(evt.details), baseTime.toISOString()]
      );
    }
    console.log(`✓ Inserted ${events.length} execution events for research #${mainResearchId}`);

    // 6. Insert mock data sources for the first research
    const sources = getSources(mainResearchId);
    for (const src of sources) {
      await client.query(
        `INSERT INTO data_sources (research_id, source_type, domain, url, status, items_found, title, description)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8)`,
        [mainResearchId, src.source_type, src.domain, src.url, src.status, src.items_found, src.title, src.description || null]
      );
    }
    console.log(`✓ Inserted ${sources.length} data sources for research #${mainResearchId}`);

    // 7. Insert mock chat history
    const sessionId = `demo-session-${mainResearchId}`;
    const chatMessages = [
      { role: 'user', message: 'What are the main findings of this research?' },
      { role: 'assistant', message: 'The research shows transformer models outperform traditional approaches by 7-15% across all NLP tasks. Key results include 96.8% accuracy on text classification and 93.2% F1 on question answering.' },
      { role: 'user', message: 'What are the limitations?' },
      { role: 'assistant', message: 'Key limitations include O(n²) complexity of self-attention, hallucination issues, significant environmental cost (~500 tonnes CO₂ for GPT-3), and potential data contamination in benchmarks.' },
    ];
    for (const msg of chatMessages) {
      await client.query(
        'INSERT INTO chat_history (session_id, user_id, role, message) VALUES ($1, $2, $3, $4)',
        [sessionId, userId, msg.role, msg.message]
      );
    }
    console.log(`✓ Inserted ${chatMessages.length} chat messages (session: ${sessionId})`);

    await client.query('COMMIT');
    console.log('\n✅ Demo account seeded successfully!');
    console.log(`   Email:    ${DEMO_EMAIL}`);
    console.log(`   Password: ${DEMO_PASSWORD}`);
    console.log(`   Researches: ${researchIds.length}`);
    console.log(`   Events: ${events.length}`);
    console.log(`   Sources: ${sources.length}`);

  } catch (err) {
    await client.query('ROLLBACK');
    console.error('❌ Seed failed:', err.message);
    throw err;
  } finally {
    client.release();
    await pool.end();
  }
}

seed().catch(() => process.exit(1));
