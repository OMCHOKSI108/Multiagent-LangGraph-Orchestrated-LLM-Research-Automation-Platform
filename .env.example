# ===========================================
# Multi-Agent LLM Research Automation Platform
# Environment Variables Configuration
# ===========================================

# ===========================================
# AI ENGINE (Python/FastAPI) Configuration
# ===========================================

# LLM Mode: "OFFLINE" (Ollama local models) or "ONLINE" (Groq cloud API)
LLM_STATUS=OFFLINE

# API Keys for Online Mode
# Groq supports multi-key rotation for rate-limit handling.
# Set as many keys as you have (at least one required for ONLINE mode).
GROQ_API_1=your_groq_api_key_1_here
GROQ_API_2=your_groq_api_key_2_here
GROQ_API_3=your_groq_api_key_3_here

# Legacy single key (still supported, used as fallback)
GROQ_API_KEY=your_groq_api_key_here

GEMINI_API_KEY=your_gemini_api_key_here
GOOGLE_SEARCH_API_KEY=your_google_search_api_key_here
GOOGLE_SEARCH_CX=your_google_search_cx_here

# Ollama Configuration (for offline mode)
OLLAMA_BASE_URL=http://localhost:11434

# Specialized Model Configuration
MODEL_REASONING=phi3:mini
MODEL_WRITING=gemma2:2b
MODEL_CODING=qwen2.5-coder:1.5b
MODEL_CRITICAL=phi3:mini

# Token Limits
MAX_TOKENS=4096

# Internal API Security (shared secret between Express backend and AI Engine)
AI_ENGINE_SECRET=change_this_to_a_secure_random_string

# ===========================================
# BACKEND (Node.js/Express) Configuration
# ===========================================

# Server Port
PORT=5000

# AI Engine URL (where the Python FastAPI server runs)
AI_ENGINE_URL=http://127.0.0.1:8000

# Internal API Security (must match AI_ENGINE_SECRET above)
AI_ENGINE_SECRET=change_this_to_a_secure_random_string

# Database Configuration (PostgreSQL)
DB_HOST=localhost
DB_PORT=5432
DB_NAME=research_platform
DB_USER=sans
DB_PASSWORD=your_db_password

# JWT Secret for Authentication
JWT_SECRET=your_jwt_secret_key_here

# ===========================================
# FRONTEND (React/Vite) Configuration
# ===========================================

# API Base URL (points to the Node.js backend)
VITE_API_BASE_URL=http://localhost:5000

# AI Engine URL (direct access from frontend for LLM status)
VITE_AI_ENGINE_URL=http://127.0.0.1:8000

# Gemini API Key (for frontend direct usage if needed)
GEMINI_API_KEY=your_gemini_api_key_here

# ===========================================
# DEVELOPMENT SETTINGS
# ===========================================

# Node Environment
NODE_ENV=development

# Python Environment (if using virtual env)
# VIRTUAL_ENV=path/to/your/venv

# ===========================================
# OPTIONAL: ADDITIONAL CONFIGURATIONS
# ===========================================

# Redis URL (if using Redis for caching)
# REDIS_URL=redis://localhost:6379

# Log Level
# LOG_LEVEL=INFO

# File Upload Limits
# MAX_FILE_SIZE=10mb

# Rate Limiting
# RATE_LIMIT_WINDOW=15
# RATE_LIMIT_MAX_REQUESTS=100
